Project Root: .

---- README.md ----
# ResNet Implementation


Implementation References
- [TinyGrad](https://github.com/tinygrad/tinygrad/blob/master/extra/models/resnet.py)
- [Torch](https://docs.pytorch.org/vision/main/_modules/torchvision/models/resnet.html)
- [Paper](https://arxiv.org/abs/1512.03385)

Requirements
- Training system
  - Hydra: Dynamic config managment
  - W&B: Logging -> Save some artifacts to it. 
  - PTL: Training wrapper; keep it only for that.
- Inference System
  - Gradio interface
  - Config system
  - ONNX compilation.
- Hydra controls full system
  - Demo
  - Model Export
  - Training
  - Ensure: Hydra is on the edge of the system, I don't want everything to be relient on it that is needed downstream in deployment.
- Basic tests
  - Stuff to make sure configs load, and model is working as expected.


## 🧪 Experiment Plan

This project explores residual networks (ResNets) under various training regimes, including full ImageNet training, fine-tuning, and CIFAR-10 replication studies. The goal is to analyze model behavior, generalization, and training dynamics across transfer learning and architecture variants.

---

### **Phase 1: ImageNet Pretraining**

**Objective**: Train a base model on ILSVRC-2012 for use in downstream transfer learning.

* **Models**:
  * `ResNet-50`
* **Setup**:
  * Standard ResNet training on ImageNet (90 epochs)
  * SGD + momentum, weight decay, multi-step LR schedule
  * Output: `resnet50_imagenet.pt`

---

### **Phase 2: CIFAR-10 Fine-Tuning**

**Objective**: Evaluate transfer learning effectiveness from ImageNet → CIFAR-10 under multiple fine-tuning strategies.

* **Base Model**: `ResNet-50` pretrained on ImageNet

#### 🔁 Fine-Tuning Variants

| Strategy     | Description                                   |
| ------------ | --------------------------------------------- |
| Head only    | Freeze backbone, train new classifier head    |
| Head + Stem  | Freeze middle layers, fine-tune low/high ends |
| Full FT      | Fine-tune entire model                        |
| FastAI-style | Discriminative LR, gradual unfreezing         |

#### 🧪 Techniques Explored

* Warm-up schedules
* Learning rate finders
* Layer freezing/unfreezing control
* BatchNorm handling during FT

---

### **Phase 3: CIFAR-10 From Scratch**

**Objective**: Replicate and validate results from He et al. (2015) for ResNet vs. PlainNet.

* **Datasets**: CIFAR-10 (32×32)
* **Models**:

  * `ResNet-{20, 32, 44, 56, 110}`
  * `Plain-{20, 32, 44, 56, 110}`
* **Training Setup**:

  * SGD + momentum, weight decay
  * LR: 0.1, decay at epochs 82 and 123
  * Epochs: 164
  * Data Augmentation: random crop, horizontal flip
* **Metrics**:

  * Training vs. validation error
  * Degradation trends
  * Layer-wise response analysis (L2 norm of activations)

---

### **Phase 4: Internal Analysis**

**Objective**: Analyze internal activations and learning behavior.

* Hook-based capture of intermediate activations
* Compute per-layer L2 norms (as in ResNet paper)
* Compare depth-wise behavior in ResNet vs. PlainNet


---- export.py ----

---- requirements.txt ----
torch
torchvision
hydra-core
lightning
numpy
omegaconf
wandb

---- src/__init__.py ----

---- src/configs/config.yaml ----
defaults:
   - _self_
   - dataset: cifar10
   - model: resnet_cifar
   - model/block: basic
   - model/stem: cifar
   - optimizer: sgd
   - scheduler: cosine
   - trainer: default
   - training: default
   - experiment: train_cifar_from_scratch

hydra:
  run:
    dir: logs/${now:%Y-%m-%d_%H-%M-%S}
  sweep:
    dir: logs/multirun/${now:%Y-%m-%d_%H-%M-%S}
  output_subdir: null
  job:
    chdir: true

---- src/configs/dataset/cifar10.yaml ----
_target_: src.data.datamodule.CIFAR10DataModule
data_dir: ${hydra:runtime.cwd}/datasets
batch_size: 256
num_workers: 7
pin_memory: true
persistent_workers: true

---- src/configs/experiment/train_cifar_from_scratch.yaml ----
experiment: train_cifar_from_scratch

---- src/configs/model/block/basic.yaml ----
_target_: src.models.blocks.BasicBlock

---- src/configs/model/resnet_cifar.yaml ----
_target_: src.models.resnet.ResNet
stem: ${model/stem}
block: ${model/block}
block_layers: [3, 3, 3]
stage_channels: [16, 32, 64]
num_classes: 10

---- src/configs/model/stem/cifar.yaml ----
_target_: src.models.stems.CIFARStem

---- src/configs/optimizer/sgd.yaml ----
_target_: torch.optim.SGD
lr: 0.1
momentum: 0.9
weight_decay: 0.0001

---- src/configs/scheduler/cosine.yaml ----
_target_: torch.optim.lr_scheduler.CosineAnnealingLR
T_max: 100
eta_min: 1e-5

---- src/configs/trainer/default.yaml ----
_target_: lightning.Trainer
max_epochs: 20
accelerator: gpu
devices: 1
strategy: auto
precision: 16-mixed
logger:
  _target_: lightning.pytorch.loggers.WandbLogger
  project: resnet_experiments
  name: ${now:%Y-%m-%d}_${experiment}
  log_model: true
callbacks:
  model_checkpoint:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    monitor: val/loss
    mode: min
    save_top_k: 1
    save_last: true
    filename: "{epoch}-{val_loss:.4f}"

---- src/configs/training/default.yaml ----
_target_: src.training.module.TrainModule
model: ${model}
optimizer: ${optimizer}
scheduler: ${scheduler}
num_classes: 10

---- src/data/__init__.py ----

---- src/data/datamodule.py ----
import lightning as L
from torch.utils.data import DataLoader
from torchvision.datasets import CIFAR10
from torchvision import transforms


class CIFAR10DataModule(L.LightningDataModule):
    def __init__(
        self,
        data_dir: str = "./datasets",
        batch_size: int = 128,
        num_workers: int = 8,
        pin_memory: bool = True,
        persistent_workers: bool = True,
    ) -> None:
        super().__init__()
        self.data_dir = data_dir
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.pin_memory = pin_memory
        self.persistent_workers = persistent_workers

        self.transform_train = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.4914, 0.4822, 0.4465],
                std=[0.2023, 0.1994, 0.2010],
            ),
        ])

        self.transform_test = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.4914, 0.4822, 0.4465],
                std=[0.2023, 0.1994, 0.2010],
            ),
        ])

    def prepare_data(self):
        CIFAR10(self.data_dir, train=True, download=True)
        CIFAR10(self.data_dir, train=False, download=True)

    def setup(self, stage=None):
        self.train_dataset = CIFAR10(
            self.data_dir, train=True, transform=self.transform_train
        )
        self.val_dataset = CIFAR10(
            self.data_dir, train=False, transform=self.transform_test
        )
    
    def train_dataloader(self):
        return DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            persistent_workers=self.persistent_workers

        )
    
    def val_dataloader(self):
        return DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            persistent_workers=self.persistent_workers

        )

---- src/data/transforms.py ----

---- src/models/__init__.py ----

---- src/models/blocks.py ----
import torch
import torch.nn as nn


class BasicBlock(nn.Module):
    expansion: int = 1

    def __init__(
        self, in_planes: int, planes: int, stride: int = 1, use_residual: bool = True
    ) -> None:
        super().__init__()
        self.use_residual = use_residual
        self.relu = nn.ReLU(inplace=True)
        self.conv1 = nn.Conv2d(
            in_channels=in_planes,
            out_channels=planes,
            kernel_size=3,
            bias=False,
            padding=1,
            stride=stride,
        )
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(
            in_channels=planes,
            out_channels=planes,
            kernel_size=3,
            bias=False,
            padding=1,
        )
        self.bn2 = nn.BatchNorm2d(planes)

        self.shortcut = nn.Identity()
        if use_residual and (stride != 1 or in_planes != planes):
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, planes, 1, stride, bias=False),
                nn.BatchNorm2d(planes),
            )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        if self.use_residual:
            out += self.shortcut(x)
        return self.relu(out)


class Bottleneck(nn.Module):
    expansion: int = 4

    def __init__(
        self, in_planes: int, planes: int, stride: int = 1, use_residual: bool = True
    ) -> None:
        super().__init__()
        self.use_residual = use_residual
        self.relu = nn.ReLU(inplace=True)

        self.conv1 = nn.Conv2d(
            in_channels=in_planes,
            out_channels=planes,
            kernel_size=1,
            bias=False,
        )
        self.bn1 = nn.BatchNorm2d(planes)
        # Technically ResNet 1.5
        self.conv2 = nn.Conv3d(
            in_channels=planes,
            out_channels=planes,
            kernel_size=3,
            stride=stride,
            padding=1,
            bias=False,
        )
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(
            in_channels=planes,
            out_channels=planes * self.expansion,
            kernel_size=1,
            bias=False,
        )
        self.bn3 = nn.BatchNorm2d(planes * self.expansion)
        self.shortcut = nn.Identity()
        if use_residual and (stride != 1 or in_planes != planes * self.expansion):
            self.shortcut = nn.Sequential(
                nn.Conv2d(
                    in_channels=in_planes,
                    out_channels=planes * self.expansion,
                    kernel_size=1,
                    stride=stride,
                    bias=False,
                ),
                nn.BatchNorm2d(planes * self.expansion),
            )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))

        if self.use_residual:
            out += self.shortcut(x)

        return self.relu(x)

---- src/models/resnet.py ----
import torch
import torch.nn as nn
from typing import List, Union, Type
from src.models.blocks import BasicBlock, Bottleneck


class ResNet(nn.Module):
    def __init__(
        self,
        stem: nn.Module,
        block: Type[Union[BasicBlock, Bottleneck]],
        block_layers: List[int],
        stage_channels: List[int],
        num_classes: int,
        use_residual: bool = True,
    ) -> None:
        super().__init__()
        self.stem = stem
        self.in_planes = stem.out_channels

        assert len(block_layers) == len(
            stage_channels
        ), "block_layers and stage_channels must align"

        self.stages = nn.ModuleList()
        for idx, (num_blocks, out_planes) in enumerate(
            zip(block_layers, stage_channels)
        ):
            stride = 1 if idx == 0 else 2
            stage = self._make_layer(
                block, out_planes, num_blocks, stride=stride, use_residual=use_residual
            )
            self.stages.append(stage)

        final_planes = stage_channels[-1] * getattr(block, "expansion", 1)
        self.pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(final_planes, num_classes)

    def _make_layer(
        self,
        block: Union[BasicBlock, Bottleneck],
        out_planes: int,
        blocks: int,
        stride: int,
        use_residual: bool,
    ) -> nn.Module:
        layers = []
        layers.append(block(self.in_planes, out_planes, stride, use_residual))
        self.in_planes = out_planes * getattr(block, "expansion", 1)
        for _ in range(1, blocks):
            layers.append(block(self.in_planes, out_planes, 1, use_residual))
        return nn.Sequential(*layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.stem(x)
        for stage in self.stages:
            x = stage(x)
        x = self.pool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x

---- src/models/stems.py ----
import torch.nn as nn


class CIFARStem(nn.Module):
    def __init__(self, out_channels: int = 16):
        super().__init__()
        self.out_channels = out_channels
        self.stem = nn.Sequential(
            nn.Conv2d(
                in_channels=3,
                out_channels=out_channels,
                kernel_size=3,
                stride=1,
                padding=1,
                bias=False,
            ),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=False),
        )

    def forward(self, x):
        return self.stem(x)

---- src/training/module.py ----
import lightning as L
import torch.nn as nn
from torchmetrics import Accuracy
from hydra.utils import instantiate
import torch

class TrainModule(L.LightningModule):
    def __init__(self, model, optimizer, scheduler, num_classes: int = 10):
        super().__init__()
        self.model = model
        self.loss_fn = nn.CrossEntropyLoss()
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.train_acc = Accuracy(task="multiclass", num_classes=num_classes)
        self.val_acc = Accuracy(task="multiclass", num_classes=num_classes)

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.loss_fn(logits, y)
        self.train_acc.update(logits, y)
        self.log("train/loss", loss, on_step=True, on_epoch=True)
        self.log("train/acc", self.train_acc, on_epoch=True)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.loss_fn(logits, y)
        self.val_acc.update(logits, y)
        self.log("val/loss", loss, on_epoch=True)
        self.log("val/acc", self.val_acc, on_epoch=True)

    def configure_optimizers(self):
        return {"optimizer": self.optimizer, "lr_scheduler": self.scheduler}

---- train.py ----
import hydra
from omegaconf import DictConfig, OmegaConf
from hydra.utils import instantiate, get_class
import lightning as L
import torch

@hydra.main(config_path="src/configs", config_name="config", version_base="1.3")
def main(cfg: DictConfig):
    block_cls = get_class(cfg.model.block._target_)
    stem = instantiate(cfg.model.stem)

    # Instantiate model with class objects
    model = instantiate(cfg.model, block=block_cls, stem=stem)
    optimizer = instantiate(cfg.optimizer, params=model.parameters())
    scheduler = instantiate(cfg.scheduler, optimizer=optimizer)

    datamodule = instantiate(cfg.dataset)
    lightning_module = instantiate(
        cfg.training,
        model=model,
        optimizer=optimizer,
        scheduler=scheduler,
    )

    # Callbacks
    callbacks = [instantiate(cb) for cb in cfg.trainer.callbacks.values()]
    logger = instantiate(cfg.trainer.logger)

    # Trainer
    trainer = L.Trainer(
        logger=logger,
        callbacks=callbacks,
        max_epochs=cfg.trainer.max_epochs,
        accelerator=cfg.trainer.accelerator,
        devices=cfg.trainer.devices,
        strategy=cfg.trainer.strategy,
        precision=cfg.trainer.precision,
        benchmark=True,
    )

    trainer.fit(lightning_module, datamodule=datamodule)

if __name__ == "__main__":
    torch.set_float32_matmul_precision("high")
    main()

